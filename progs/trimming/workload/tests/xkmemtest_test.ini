# Default module_template configuration.
# Lines beginning with '#' are comments; blank lines are ignored

[defaults]
work_root: ~/workload_test_suite/xkmemtest_test
mode: interactive
wlm: 
job_launcher: alps
#units -  m:minutes, h:hours
wall_clock_time_limit_unit:m
wall_clock_time_limit_value:

[xkmemtest_test]
component_test_type:nvidia
test_script:xkmemtest
test_script_parameters:--num_iterations 1000 --num_passes 4 
test_script_parameters_p100:--num_passes 4 --num_iterations 1000

test_script_source_dir:/opt/cray/diag/bin/nvidia
test_script_ini_source_dir:/opt/cray/diag/etc
test_script_ini:
additional_ld_library_path_components:

number_of_nodes_per_job:4
run_test_copies_concurrently:True
supported_accelerator_names:Tesla_K20X,Tesla_K40s,Tesla_P100-PCIE-16GB,Tesla_P100-PCIE-12GB

#log parsing details
error_keyword_list:
error_match_exclusion_list:
single_logfile_per_job:True
nodeinfo_in_logfile_name:True
timestamp_path_index:-2
timestamp_component_index:-1
nodeinfo_path_index:-1
nodeinfo_component_index:3
nodeinfo_component_index_alternate:-1

[alps]
#default aprun settings 
job_launcher:aprun 
aprun_parameters:-n WIDTH -N 1 -L NODE_LIST

[slurm]
job_launcher:srun 
#srun_parameters:--exclusive --gres=gpu -n WIDTH --ntasks-per-node=NPPN --nodelist=NODE_LIST --time=90
srun_parameters:--exclusive --gres=gpu -n WIDTH --ntasks-per-node=NPPN --nodelist=NODE_LIST --time=360
